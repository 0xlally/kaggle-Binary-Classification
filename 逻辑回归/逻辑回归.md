[toc]

# 选用sklearn的原因

### 数据的“长相”决定了工具的选择

**非结构化数据（Unstructured Data） -> PyTorch/TensorFlow 的主场**

- 比如：**图片、音频、文本（自然语言）。**
- **特点**：特征之间有极强的空间或时间相关性（比如像素点组成了边缘，边缘组成了猫耳朵）。
- **PyTorch 的强项**：深度神经网络（CNN, Transformer）擅长从这些模糊的数据中提取高阶特征。

**结构化数据（Structured/Tabular Data） -> Sklearn/GBDT 的主场**

- 比如：**你的这个银行数据集**（年龄、职业、余额、是否有房贷）。
- **特点**：每一列的物理意义完全不同，特征之间没有像像素那样明显的空间关系。“年龄”和“余额”放在一起，不需要卷积，而是需要逻辑判断。
- **树模型的强项**：决策树天然适合处理这种“异质”数据。

### 算法原理的冲突：离散 vs 连续

**PyTorch 的核心是“梯度下降” (Gradient Descent)**：

- 它要求每一个操作都是**可导的**（光滑的）。
- 神经网络是通过微调权重（比如把 0.5 变成 0.5001）来慢慢逼近结果。

**决策树/随机森林的核心是“硬切分” (Hard Split)**：

- 逻辑是：`如果 年龄 > 30，走左边；否则，走右边`。
- 这是一个**阶跃函数**（Step Function）。在数学上，这个操作在切分点不可导（导数为无穷大或无定义），在其他地方导数为 0。
- **结论**：因为没法求导，所以 PyTorch 最强大的“自动反向传播”机制对标准决策树**完全失效**。虽然有“神经决策树”这种变体，但效果通常不如传统的树模型。

# sklearn实现

## 数据处理

panda读取

```
train_df = pd.read_csv('../data/train.csv')
```

将数值列和字符列分开

```
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X.select_dtypes(include=['object']).columns
```

数值列处理：中位数填充缺失值 + 标准化

```
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')), 
    ('scaler', StandardScaler())
])
```

类别列处理：unknown填充缺失值 + One-Hot编码 (遇到新类别忽略)

```
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')), # 保持 unknown 为一类
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])
```

构建处理管道

```
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# --- 3. 定义逻辑回归模型 ---
# max_iter=2000 保证有足够时间收敛，C=1.0 是正则化强度默认值
model = LogisticRegression(max_iter=2000, random_state=42, C=1.0, solver='lbfgs')

# --- 4. 构建最终管道 ---
clf = Pipeline(steps=[('preprocessor', preprocessor),
                      ('classifier', model)])
```

12折交叉验证及计算roc-auc

```
# 实例化切分器
cv = StratifiedKFold(n_splits=12, shuffle=True, random_state=42)

# 计算 AUC
scores = cross_val_score(clf, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)
```

保存模型

```
# --- 7. 在全部训练数据上训练最终模型 ---
print("\n训练最终模型...")
clf.fit(X, y)
print("模型训练完成！")

# --- 8. 保存模型 ---
model_path = 'logistic_regression_model.pkl'
joblib.dump(clf, model_path)
print(f"\n模型已保存到: {model_path}")
```



## pytorch实现

在做的过程中，我们发现用pytorch处理表格数据很麻烦，很多处理数据都不如调用sklearn的现成的库函数，最后无非是定义了一个线性层模型。所以不如直接用sklearn去进行调优。
