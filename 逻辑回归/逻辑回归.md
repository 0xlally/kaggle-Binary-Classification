[toc]

# 选用sklearn的原因

### 数据的“长相”决定了工具的选择

**非结构化数据（Unstructured Data） -> PyTorch/TensorFlow 的主场**

- 比如：**图片、音频、文本（自然语言）。**
- **特点**：特征之间有极强的空间或时间相关性（比如像素点组成了边缘，边缘组成了猫耳朵）。
- **PyTorch 的强项**：深度神经网络（CNN, Transformer）擅长从这些模糊的数据中提取高阶特征。

**结构化数据（Structured/Tabular Data） -> Sklearn/GBDT 的主场**

- 比如：**你的这个银行数据集**（年龄、职业、余额、是否有房贷）。
- **特点**：每一列的物理意义完全不同，特征之间没有像像素那样明显的空间关系。“年龄”和“余额”放在一起，不需要卷积，而是需要逻辑判断。
- **树模型的强项**：决策树天然适合处理这种“异质”数据。

### 算法原理的冲突：离散 vs 连续

**PyTorch 的核心是“梯度下降” (Gradient Descent)**：

- 它要求每一个操作都是**可导的**（光滑的）。
- 神经网络是通过微调权重（比如把 0.5 变成 0.5001）来慢慢逼近结果。

**决策树/随机森林的核心是“硬切分” (Hard Split)**：

- 逻辑是：`如果 年龄 > 30，走左边；否则，走右边`。
- 这是一个**阶跃函数**（Step Function）。在数学上，这个操作在切分点不可导（导数为无穷大或无定义），在其他地方导数为 0。
- **结论**：因为没法求导，所以 PyTorch 最强大的“自动反向传播”机制对标准决策树**完全失效**。虽然有“神经决策树”这种变体，但效果通常不如传统的树模型。

# sklearn实现

## 数据处理

panda读取

```
train_df = pd.read_csv('../data/train.csv')
```

将数值列和字符列分开

```
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X.select_dtypes(include=['object']).columns
```

数值列处理：中位数填充缺失值 + 标准化

```
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')), 
    ('scaler', StandardScaler())
])
```

类别列处理：unknown填充缺失值 + One-Hot编码 (遇到新类别忽略)

```
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')), # 保持 unknown 为一类
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])
```

构建处理管道

```
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# --- 3. 定义逻辑回归模型 ---
# max_iter=2000 保证有足够时间收敛，C=1.0 是正则化强度默认值
model = LogisticRegression(max_iter=2000, random_state=42, C=1.0, solver='lbfgs')

# --- 4. 构建最终管道 ---
clf = Pipeline(steps=[('preprocessor', preprocessor),
                      ('classifier', model)])
```

12折交叉验证及计算roc-auc

```
# 实例化切分器
cv = StratifiedKFold(n_splits=12, shuffle=True, random_state=42)

# 计算 AUC
scores = cross_val_score(clf, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)
```

保存模型

```
# --- 7. 在全部训练数据上训练最终模型 ---
print("\n训练最终模型...")
clf.fit(X, y)
print("模型训练完成！")

# --- 8. 保存模型 ---
model_path = 'logistic_regression_model.pkl'
joblib.dump(clf, model_path)
print(f"\n模型已保存到: {model_path}")
```

初步实现完整代码

```
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
import pandas as pd
import joblib  # 用于保存模型

# 读取训练集数据
# 注意：这里的路径 '../data/train.csv' 需要根据你实际文件的位置修改
train_df = pd.read_csv('../data/train.csv')

# --- 1. 准备数据 ---
X = train_df.drop(columns=['id', 'y']) 
y = train_df['y']

# --- 2. 定义预处理逻辑 ---
# 自动识别列类型
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X.select_dtypes(include=['object']).columns

# 数值列处理：中位数填充缺失值 + 标准化
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')), 
    ('scaler', StandardScaler())
])

# 类别列处理：unknown填充缺失值 + One-Hot编码 (遇到新类别忽略)
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')), # 保持 unknown 为一类
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

# 组合起来
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# --- 3. 定义逻辑回归模型 ---
# max_iter=2000 保证有足够时间收敛，C=1.0 是正则化强度默认值
model = LogisticRegression(max_iter=2000, random_state=42, C=1.0, solver='lbfgs')

# --- 4. 构建最终管道 ---
clf = Pipeline(steps=[('preprocessor', preprocessor),
                      ('classifier', model)])

# --- 5. 执行 12折交叉验证 ---
print("开始执行 12-Fold Cross Validation (这可能需要几秒钟)...")

# 实例化切分器
cv = StratifiedKFold(n_splits=12, shuffle=True, random_state=42)

# 计算 AUC
scores = cross_val_score(clf, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)

# --- 6. 输出结果 ---
print(f"\n每折的 AUC 分数:\n{scores}")
print(f"\n>>> 平均 AUC: {scores.mean():.5f} (标准差: {scores.std():.5f})")

# --- 7. 在全部训练数据上训练最终模型 ---
print("\n训练最终模型...")
clf.fit(X, y)
print("模型训练完成！")

# --- 8. 保存模型 ---
model_path = 'logistic_regression_model.pkl'
joblib.dump(clf, model_path)
print(f"\n模型已保存到: {model_path}")
```

## 继续优化

### 什么是特征工程？

如果把机器学习比作**做饭**：

- **数据 (Data)** = 原始食材（带泥的土豆、整只鸡）。
- **模型 (Model)** = 烹饪设备（电饭煲、烤箱）。
- **特征工程 (Feature Engineering)** = **备菜**（洗净、切块、腌制、搭配）。

**为什么逻辑回归特别需要特征工程？** 因为逻辑回归（LR）是一个“线性模型”，它的思维非常简单、直白。

- 它只能理解：“年龄越大，存款概率越高”这种直线关系。
- 它**理解不了**：“30-50岁的人存款概率高，但60岁以上反而变低了”（这是曲线）。
- 它**理解不了**：“有房贷”且“失业”的人风险极高（这是交互关系）。

**特征工程就是把这些复杂的逻辑，手动算好，喂给逻辑回归吃。**

### 针对银行营销数据的特征工程策略

为了把 AUC 从 0.74 往上提，我们需要针对这个数据集做以下 3 个具体的“手术”：

#### 1. 处理 `pdays` (上次联系天数) —— 解决“魔法数字”

- **问题**：数据里 `pdays` = -1 表示“以前从未联系过”。但在数学上，-1 是一个很小的数字，模型会以为 -1 比 100 小很多。这完全误导了逻辑回归！其实 -1 代表的是一种“状态”，而不是“天数”。
- **对策**：新增一列 `was_contacted`（是否曾联系过）。如果 pdays = -1，则是 0；否则是 1。

#### 2. 年龄分箱 (Binning) —— 解决“非线性”

- **问题**：逻辑回归认为 60 岁的影响力是 30 岁的 2 倍，这不科学。通常刚工作的年轻人和退休老人没钱，中年人有钱。
- **对策**：把 `age` 切成几段：`[0-30, 30-40, 40-50, 50-60, 60+]`。把连续数字变成类别，逻辑回归就能对每一段年龄赋予不同的权重了。

#### 3. 交互特征 (Interaction) —— 解决“关联性”

- **问题**：单看 `housing` (有房贷) 可能影响不大，单看 `loan` (有个人贷款) 也还好。但如果一个人**同时**有房贷和个人贷款，他的经济压力可能爆表，几乎不可能存钱。
- **对策**：创造一个新特征 `total_debt` = `housing` + `loan`。

优化代码

```
import pandas as pd
import numpy as np
import joblib  # 核心库：用于保存和加载模型
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, KBinsDiscretizer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression

# 1. 读取数据
# 请确保路径正确
train_df = pd.read_csv('../data/train.csv')

# ==========================================
#  函数：特征工程 (Feature Engineering)
#  注意：这个函数以后对 test.csv 也要用！
# ==========================================
def feature_engineering(df):
    df_eng = df.copy()
    
    # 1. 处理 pdays (-1 代表从未联系)
    df_eng['was_contacted'] = (df_eng['pdays'] != -1).astype(int)
    
    # 2. 交互特征: 总负债状况
    housing_num = df_eng['housing'].map({'yes': 1, 'no': 0, 'unknown': 0})
    loan_num = df_eng['loan'].map({'yes': 1, 'no': 0, 'unknown': 0})
    df_eng['debt_level'] = housing_num + loan_num
    
    return df_eng

# 对训练集应用特征工程
print("正在进行特征工程...")
train_df_eng = feature_engineering(train_df)

# ==========================================
#  准备数据 X 和 y
# ==========================================
X = train_df_eng.drop(columns=['id', 'y']) 
y = train_df_eng['y']

# 定义列名 (根据特征工程后的新列调整)
numeric_features = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous', 'debt_level', 'was_contacted']
categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']

# ==========================================
#  构建 Pipeline
# ==========================================
# 数值处理
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# 年龄分箱 (把年龄变成类别，捕捉非线性)
age_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('binning', KBinsDiscretizer(n_bins=10, encode='onehot', strategy='quantile'))
])

# 类别处理
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

# 组合处理器
preprocessor = ColumnTransformer(
    transformers=[
        ('age_bin', age_transformer, ['age']), # 单独处理 age
        ('num', numeric_transformer, [c for c in numeric_features if c != 'age']), # 其他数值
        ('cat', categorical_transformer, categorical_features) # 类别
    ])

# 定义逻辑回归模型 (带 class_weight='balanced')
model = LogisticRegression(
    max_iter=3000, 
    C=0.1, 
    solver='lbfgs',
    class_weight='balanced', # 关键参数
    random_state=42
)

# 最终管道
clf = Pipeline(steps=[('preprocessor', preprocessor),
                      ('classifier', model)])

# ==========================================
#  步骤 1: 交叉验证 (评估模型好坏)
# ==========================================
print("正在执行 12-Fold Cross Validation 评估...")
cv = StratifiedKFold(n_splits=12, shuffle=True, random_state=42)
scores = cross_val_score(clf, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)

print(f"每折 AUC: {scores}")
print(f">>> 平均 AUC: {scores.mean():.5f} (标准差: {scores.std():.5f})")

# ==========================================
#  步骤 2: 全量训练 (生成最终模型)
# ==========================================
print("\n正在使用所有训练数据训练最终模型...")
clf.fit(X, y)
print("模型训练完成！")

# ==========================================
#  步骤 3: 保存模型
# ==========================================
model_filename = 'logistic_regression_optimized.pkl'
joblib.dump(clf, model_filename)
print(f"模型已成功保存为: {model_filename}")

# ==========================================
#  附加步骤: 演示如何读取模型并生成提交文件
# ==========================================
print("\n--- 模拟预测 Test 集流程 ---")

# 1. 读取 Test 数据
test_df = pd.read_csv('../data/test.csv')

# 2. 重要！必须对 Test 集做同样的特征工程
test_df_eng = feature_engineering(test_df)

# 3. 加载模型 (其实可以直接用上面的 clf，这里为了演示加载流程)
loaded_model = joblib.load(model_filename)

# 4. 预测概率 (注意用 predict_proba 获取概率，取第2列即为正类概率)
# 不需要再手动调预处理，loaded_model 里的 pipeline 会自动处理
test_pred_prob = loaded_model.predict_proba(test_df_eng)[:, 1]

# 5. 生成提交 DataFrame
submission = pd.DataFrame({
    'id': test_df['id'],
    'y': test_pred_prob
})

print("预测完成，前 5 行预览：")
print(submission.head())

submission.to_csv('submission2.csv', index=False)
```

这里注意一个细节，gemini不提醒还真想不到，kaggle要的其实不是0或者1的分类，而是预测的概率值，所以其实输出概率提交分数反而会更高，不信的话可以验证一下，只需要改几行代码即可

```
# 修改为 predict，直接输出类别
test_pred_labels = model.predict(test_df_eng)

submission_binary = pd.DataFrame({
    'id': test_df['id'],
    'y': test_pred_labels
})
submission_binary.to_csv('submission_binary.csv', index=False)
```

到这一步准确率其实就已经很高了，**我们梳理一下做了哪些优化**

![image-20251207170201909](https://image-hub.oss-cn-chengdu.aliyuncs.com/image-20251207170201909.png)

### 1. 特征工程层面 (Feature Engineering) —— 最核心的提升

逻辑回归太“耿直”，不懂弯弯绕绕，我们通过手动构造特征帮它“理解”数据：

- **修复逻辑漏洞 (`pdays`)**：
  - **优化前**：`-1` 被当作数值处理，模型认为它比 `0` 小，这在业务上是错的。
  - **优化后**：新增 `was_contacted`（是否联系过），把 `-1` 的特殊含义剥离出来变成布尔值。
- **捕捉非线性关系 (Age Binning)**：
  - **优化前**：逻辑回归认为年龄越大越好（或越差），是直线关系。
  - **优化后**：使用 `KBinsDiscretizer` 将年龄分箱（如 20-30岁一档，30-40岁一档）。这让线性模型也能拟合出“U型”或“倒U型”的复杂曲线（例如：年轻人和老人没钱，中年人有钱）。
- **构建交互特征 (`debt_level`)**：
  - **优化前**：房贷 (`housing`) 和个贷 (`loan`) 各自独立。
  - **优化后**：将两者相加生成“总负债等级”。这帮模型捕捉到了“多重负债导致存款意愿极低”的组合效应。

### 2. 模型策略层面 (Model Strategy) —— 针对短板

针对逻辑回归的数学特性和数据的分布特性做了调整：

- **对抗类别不平衡 (`class_weight='balanced'`)**：
  - **原因**：银行营销数据中，只有少数人会存款（正样本少）。普通模型会倾向于预测“不存款”来混高准确率。
  - **优化**：强制模型给“正样本”更高的权重，宁可错杀（误报），不可放过（漏报），这直接提升了 AUC。
- **强制数值归一化 (`StandardScaler`)**：
  - **原因**：`balance`（余额）的数值很大（几千几万），`age` 很小（几十）。如果不处理，逻辑回归会完全被余额主导，忽略年龄。
  - **优化**：将所有数值拉回同一个起跑线（均值0，方差1），让所有特征公平竞争。
- **加强正则化 (`C=0.1`)**：
  - **原因**：我们做 One-Hot 和分箱后，特征维度变多了，容易过拟合。
  - **优化**：降低 `C` 值（增强惩罚力度），让模型参数更保守，泛化能力更强。

### 3. 工程与评估层面 (Engineering & Workflow) —— 确保结果可靠

- **12折交叉验证 (12-Fold CV)**：
  - 不再依赖单一的随机划分，而是用 12 轮测试的平均分来评估，确保你的分数不是“运气好”碰上的。
- **Pipeline 管道封装**：
  - 将“填充缺失值 -> 归一化 -> 分箱 -> 训练”封装成一个整体。这确保了预测 Test 集时，使用的是和训练集**完全一致**的数据处理标准（防止数据泄露）。
- **全量训练与保存 (Fit All & Save)**：
  - 明确了交叉验证只是为了“体检”，真正提交时，必须用所有数据重新 `fit` 一次，并用 `joblib` 保存模型，完成了从实验到落地的闭环。
- **纠正提交格式 (`predict_proba`)**：
  - 明确了竞赛提交的是**概率值**而非类别（0/1），这是获得高 AUC 分数的关键一步。

### 正则化

**正则化 = 让模型更简单，避免过拟合。**
 **L2 正则 = 惩罚权重平方，让权重更小、模型更稳定。**
 **C 越小 = 正则化越强 = 模型越不容易过拟合。**

#### 为什么 L2 正则能防止过拟合？

##### ✨ 原因：让模型不敢把某个特征的系数学得太大

如果没有正则化：

- 模型可能会“特别相信某几个特征”
- 导致权重变得特别大
- 非常容易被噪声数据诱导 → 过拟合

## 遇到的问题

### **数据进行特征工程后，原先的列应该保存还是去除**

### 情况 1：分箱特征 (Binning) —— 比如 `age` vs `age_bin`

**操作**：你把连续的 `age` (25, 30, 35...) 变成了离散的 `age_bin` (20-30区间, 30-40区间...)。

**对于逻辑回归 (Linear Model)**：

- **建议**：**通常只保留其中一个**（我之前的代码里是去掉了原始 `age`，只留了分箱后的）。
- **原因**：
  - 如果你保留两者，会产生**共线性 (Multicollinearity)**。因为 `age` 和 `age_bin` 是高度相关的。逻辑回归不喜欢两个特征说同一件事，这会导致权重（系数）乱跳。
  - **但在一种情况下可以保留两者**：如果你认为“年龄”既有一个整体的线性趋势（年纪越大越有钱），又有局部的非线性波动（30岁是一个坎）。同时保留两者可以让模型学习“整体趋势 + 局部修正”。

**对于树模型 (XGBoost/LightGBM)**：

- **建议**：**保留两者**。树模型非常聪明，它会自己在 `age` 和 `age_bin` 之间选一个更好用的切分点。

### 情况 2：组合特征 (Interaction) —— 比如 `debt_level` vs `housing` / `loan`

- **操作**：`debt_level` = `housing` + `loan`。
- **对于逻辑回归**：
  - **建议**：**全部保留**。
  - **原因**：
    - `housing` 代表“房贷类型”，`loan` 代表“个贷类型”。
    - `debt_level` 代表“负债总强度”。
    - 这三个特征虽然数学上相关，但在业务含义上是不同的维度。
    - 逻辑回归有 **正则化 (Regularization, 参数 C)**。如果 `debt_level` 和 `housing` 功能重复了，L2 正则化会自动把其中一个不重要的特征权重压低。所以，**把它们都扔进去，让算法自己去挑**是更稳妥的策略。

### 情况 3：提取特征 (Extraction) —— 比如 `was_contacted` vs `pdays`

- **操作**：从 `pdays` 里提取出了 `was_contacted`。
- **对于逻辑回归**：
  - **建议**：**全部保留，但要小心原始列**。
  - **原因**：
    - `was_contacted` 解决了“有没有联系”的问题。
    - `pdays` 解决了“联系了多久”的问题。
    - 它们互为补充。**但是！** 原始的 `pdays` 里含有 `-1` 这个魔法数字，这对逻辑回归是有毒的。
  - **最佳做法**：
    1. 生成 `was_contacted`。
    2. **清洗** `pdays`：把 `-1` 变成一个很大的数字（比如 999）或者变成 0（根据业务理解），或者直接把 `-1` 的那些行在 `pdays` 列里设为平均值。
    3. 然后把两个都扔进模型。
