# kaggle-Binary-Classification
来源于kaggle playground的Binary Classification with a Bank Dataset，地址：https://www.kaggle.com/competitions/playground-series-s5e8
在这个系列中，我们将学习使用逻辑回归，决策树，随机森林，梯度提升树进行二分类

## 数据集处理

kaggle提交去验证效率太低，所以我们在训练阶段从训练集中分层抽样划分出一个验证集。

在这个方向更有效的是k折交叉验证，它会把数据分成 K 份（比如 5 份），轮流做验证集，最后取平均分。这对随机森林和梯度提升树（GBDT/XGBoost/LightGBM）特别有效。

### AUC-POC

**AUC-ROC** 是二分类问题中最常用的评估指标，尤其是在 Kaggle 竞赛和工业界（如金融风控、医疗诊断）中，它被称为**“黄金标准”**。

简单来说，它不看你预测的“具体类别”（0还是1），而是评估你预测出的**“概率值”靠不靠谱**。

**为什么要用它？**

要做“银行营销”预测（预测客户是否会存钱），这通常是一个**类别不平衡**的数据集（比如 90% 的人不会存钱，只有 10% 的人会）。

- **如果只看准确率 (Accuracy)**：模型只要无脑全部预测“不会存钱”，准确率就能高达 90%。但这个模型是废的，因为它一个客户也没抓到。
- **AUC 的优势**：它不关心你具体的 0/1 判定阈值是多少，它只关心**你能不能把“正样本”的概率排在“负样本”前面**。

**TPR (真阳性率/召回率)**：所有真的正样本中，你找出了多少？（比如：真的会存钱的客户，你抓住了多少？）-> **越大越好**

**FPR (假阳性率/误报率)**：所有真的负样本中，你误判了多少？（比如：明明不会存钱的客户，你却骚扰了多少？）-> **越小越好**

**ROC 曲线的绘制过程**： 模型输出的是概率（比如 0.1, 0.6, 0.9）。为了决定谁是 1，谁是 0，我们需要一个**阈值 (Threshold)**。

- 如果阈值设为 0（所有人都是 1）：我们抓住了所有坏人（TPR=1），但也骚扰了所有好人（FPR=1）。
- 如果阈值设为 1（所有人都是 0）：我们谁也没抓到（TPR=0），也没骚扰任何人（FPR=0）。
- **ROC 曲线**就是让阈值从 0 慢慢滑动到 1，把中间产生的所有 (FPR, TPR) 点连成的线。

### 模型选择

**非结构化数据（Unstructured Data） -> PyTorch/TensorFlow 的主场**

- 比如：**图片、音频、文本（自然语言）。**
- **特点**：特征之间有极强的空间或时间相关性（比如像素点组成了边缘，边缘组成了猫耳朵）。
- **PyTorch 的强项**：深度神经网络（CNN, Transformer）擅长从这些模糊的数据中提取高阶特征。

**结构化数据（Structured/Tabular Data） -> Sklearn/GBDT 的主场**

- 比如：**你的这个银行数据集**（年龄、职业、余额、是否有房贷）。
- **特点**：每一列的物理意义完全不同，特征之间没有像像素那样明显的空间关系。“年龄”和“余额”放在一起，不需要卷积，而是需要逻辑判断。
- **树模型的强项**：决策树天然适合处理这种“异质”数据。



**PyTorch 的核心是“梯度下降” (Gradient Descent)**：

- 它要求每一个操作都是**可导的**（光滑的）。
- 神经网络是通过微调权重（比如把 0.5 变成 0.5001）来慢慢逼近结果。

**决策树/随机森林的核心是“硬切分” (Hard Split)**：

- 逻辑是：`如果 年龄 > 30，走左边；否则，走右边`。
- 这是一个**阶跃函数**（Step Function）。在数学上，这个操作在切分点不可导（导数为无穷大或无定义），在其他地方导数为 0。
- **结论**：因为没法求导，所以 PyTorch 最强大的“自动反向传播”机制对标准决策树**完全失效**。虽然有“神经决策树”这种变体，但效果通常不如传统的树模型。
